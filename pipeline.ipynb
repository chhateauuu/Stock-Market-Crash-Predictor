{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11147fd",
   "metadata": {},
   "source": [
    "This file will cover the entire data pipeline process for acquiring all the data, cleaning, and processing for the sake of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d2a93",
   "metadata": {},
   "source": [
    "# Importing and configurations, also folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19809636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code\n",
      "Data directory: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data\n",
      "Raw data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw\n",
      "Cleaned data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned\n",
      "Merged data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/merged\n"
     ]
    }
   ],
   "source": [
    "import os # Note: for working with folders / paths\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf             # Note: To download stock/market data\n",
    "from fredapi import Fred          # Note: To download macroeconomic data from FRED\n",
    "from pytrends.request import TrendReq  # Note: To pull Google Trends sentiment data\n",
    "import duckdb                     # Note: This one for now is unsure-ish, can be used for storing/querying data with SQL\n",
    "\n",
    "# Note: This is the BASE_DIR = the folder where this notebook is running\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# # Note: This will be the main data directory:\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Note: Subfolders for different stages of the pipeline:\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "CLEANED_DIR = os.path.join(DATA_DIR, \"cleaned\")\n",
    "MERGED_DIR = os.path.join(DATA_DIR, \"merged\")\n",
    "\n",
    "# Note: Now also creating more specific subfolders for raw data, each housing different stuff:\n",
    "# Note: data/raw/prices/     -> raw price data (S&P 500, VIX, etc.)\n",
    "# Note: data/raw/macro/      -> raw macro data (CPI, unemployment, etc.)\n",
    "# Note: data/raw/sentiment/  -> raw sentiment data (Google Trends, etc.)\n",
    "RAW_PRICES_DIR = os.path.join(RAW_DIR, \"prices\")\n",
    "RAW_MACRO_DIR = os.path.join(RAW_DIR, \"macro\")\n",
    "RAW_SENTIMENT_DIR = os.path.join(RAW_DIR, \"sentiment\")\n",
    "\n",
    "# Note: Same structure for cleaned data:\n",
    "# Note: data/cleaned/prices/\n",
    "# Note: data/cleaned/macro/\n",
    "# Note: data/cleaned/sentiment/\n",
    "CLEAN_PRICES_DIR = os.path.join(CLEANED_DIR, \"prices\")\n",
    "CLEAN_MACRO_DIR = os.path.join(CLEANED_DIR, \"macro\")\n",
    "CLEAN_SENTIMENT_DIR = os.path.join(CLEANED_DIR, \"sentiment\")\n",
    "\n",
    "# Note: We want historical data starting from 1980, so setting that as a golbal variable\n",
    "START_DATE = \"1980-01-01\"\n",
    "\n",
    "# Note: Today's date (so we always pull up-to-date data)\n",
    "TODAY = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Note: These will be the financial tickers (for yfinance)\n",
    "# Note: ^GSPC = S&P 500 index; ^VIX  = Volatility Index\n",
    "TICKERS = {\n",
    "    \"sp500\": \"^GSPC\",\n",
    "    \"vix\": \"^VIX\",\n",
    "}\n",
    "\n",
    "# Note: Next we have the FRED macroeconomic series IDs\n",
    "# Note: These IDs come from the FRED website\n",
    "FRED_SERIES = {\n",
    "    \"cpi\": \"CPIAUCSL\",           # Note: Consumer Price Index\n",
    "    \"unemployment\": \"UNRATE\",    # Note: Unemployment rate\n",
    "    \"fed_funds_rate\": \"FEDFUNDS\",\n",
    "    \"m2_money_stock\": \"M2SL\",\n",
    "}\n",
    "\n",
    "# Note: Finally the Google Trends sentiment settings\n",
    "# Note: These are some of the keywords we care about for sentiment around crashes/recessions.\n",
    "TRENDS_KEYWORDS = [\n",
    "    # \"recession\",\n",
    "    # \"stock market crash\",\n",
    "    \"bear market\",\n",
    "    \"financial crisis\",\n",
    "    \"economic crisis\",\n",
    "    \"market crash\",\n",
    "    \"economic collapse\",\n",
    "    \"market panic\",\n",
    "    \"market volatility\",\n",
    "    \"stock crash\",\n",
    "    \"credit crunch\",\n",
    "    \"bank run\",\n",
    "    \"bank collapse\",\n",
    "    \"investor panic\",\n",
    "    \"yield curve inversion\",\n",
    "    \"inflation crisis\",\n",
    "    \"stagflation\",\n",
    "    \"should I sell my stocks\",\n",
    "    \"why is the stock market dropping\",\n",
    "    \"market uncertainty\",\n",
    "    \"fed meeting\",\n",
    "    \"inflation report\",\n",
    "    \"jobs report\"\n",
    "]\n",
    "# Note: Google Trends only goes back to 2004, so our sentiment data starts then.\n",
    "TRENDS_START_DATE = \"2004-01-01\"\n",
    "\n",
    "# Note: Let's print it so we can make sure most of this configuration and folder setup worked\n",
    "print(\"Base directory:\", BASE_DIR)\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "print(\"Raw data folder:\", RAW_DIR)\n",
    "print(\"Cleaned data folder:\", CLEANED_DIR)\n",
    "print(\"Merged data folder:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f5340",
   "metadata": {},
   "source": [
    "# Acquire price data (S&P 500 + VIX) by downloading all of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a64fcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/q8ls960x2513d35yd59cvhc00000gn/T/ipykernel_88777/2921958070.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/var/folders/bv/q8ls960x2513d35yd59cvhc00000gn/T/ipykernel_88777/2921958070.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prices] Downloading sp500 (^GSPC) from 1980-01-01 to 2025-11-12 ...\n",
      "[prices] Saved sp500 data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/prices/sp500_prices_raw.csv (rows: 11561)\n",
      "[prices] Downloading vix (^VIX) from 1980-01-01 to 2025-11-12 ...\n",
      "[prices] Saved vix data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/prices/vix_prices_raw.csv (rows: 9033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: This is where we will download the actual price data using yfinance, this includes the S&P 500 (^GSPC) and the VIX index (^VIX)\n",
    "# Note: We'll pull all data from our START_DATE (1980) up through TODAY\n",
    "# Note: The print statements below are practices we are adopting so as to make sure at each step we understand what is causing an error if something happens for debugging\n",
    "\n",
    "def download_prices(name, ticker, start=START_DATE, end=TODAY):\n",
    "    # Note: This function downloads price data for a given ticker.\n",
    "    # Note: 'name' is just our friendly label like \"sp500\".\n",
    "    # Note: 'ticker' is the actual Yahoo Finance symbol like \"^GSPC\".\n",
    "\n",
    "    print(f\"[prices] Downloading {name} ({ticker}) from {start} to {end} ...\")\n",
    "\n",
    "    # Note: yf.download() pulls OHLCV data (Open, High, Low, Close, Volume)\n",
    "    df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "    # Note: If somehow yfinance returns nothing, we don't want the code to break.\n",
    "    if df.empty:\n",
    "        print(f\"[prices] WARNING: no data returned for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    # Note: So first gotta convert all column names into plain strings because one of the column names in yfinance seem to not be str but a tuple\n",
    "    df.columns = df.columns.map(str)\n",
    "\n",
    "    # Note: Moving the index into a normal 'date' column and cleaning column names a bit.\n",
    "    df = df.reset_index()\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_prices(df, name):\n",
    "    # Note: This function simply saves the dataframe to the correct folder.\n",
    "    # Note: If df is None or empty, then there's nothing to save.\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(f\"[prices] Nothing to save for {name}\")\n",
    "        return\n",
    "\n",
    "    # Note: Construct the path like: data/raw/prices/sp500_prices_raw.csv\n",
    "    out_path = os.path.join(RAW_PRICES_DIR, f\"{name}_prices_raw.csv\")\n",
    "\n",
    "    # Note: Saving all of the data\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"[prices] Saved {name} data to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: Now we loop through all tickers we defined earlier (S&P 500 + VIX)\n",
    "# Note: For each ticker, we download it and then save it.\n",
    "for name, ticker in TICKERS.items():\n",
    "    df = download_prices(name, ticker)\n",
    "    save_prices(df, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a61d5",
   "metadata": {},
   "source": [
    "# Get the macro data from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2dfc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to FRED\n",
      "[macro] Downloading FRED series CPIAUCSL from 1980-01-01 to 2025-11-12 ...\n",
      "Saved cpi data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/cpi_fred_raw.csv (rows: 549)\n",
      "[macro] Downloading FRED series UNRATE from 1980-01-01 to 2025-11-12 ...\n",
      "Saved unemployment data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/unemployment_fred_raw.csv (rows: 548)\n",
      "[macro] Downloading FRED series FEDFUNDS from 1980-01-01 to 2025-11-12 ...\n",
      "Saved fed_funds_rate data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/fed_funds_rate_fred_raw.csv (rows: 550)\n",
      "[macro] Downloading FRED series M2SL from 1980-01-01 to 2025-11-12 ...\n",
      "Saved m2_money_stock data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/m2_money_stock_fred_raw.csv (rows: 549)\n"
     ]
    }
   ],
   "source": [
    "# Note: For loading the .env variables like API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Note: Already went to the FRED website and got an API key, which is in the .env (so git won't track it), so will use that for this step\n",
    "# Note: Now we move on to downloading macroeconomic data from FRED\n",
    "# Note: This will give us things like CPI, unemployment, Fed funds rate, and M2 money stock\n",
    "\n",
    "# Note: Just pulling that API key from the .env\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Note: Now we will create the FRED client using the key\n",
    "fred = None\n",
    "try:\n",
    "    fred = Fred(api_key=FRED_API_KEY)\n",
    "    print(\"Successfully connected to FRED\")\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to FRED:\", e)\n",
    "    fred = None\n",
    "\n",
    "\n",
    "def download_fred_series(series_id, start=START_DATE, end=TODAY):\n",
    "    # Note: This function downloads ONE macro time series from FRED, example series_id values would be like \"CPIAUCSL\", \"UNRATE\", etc\n",
    "\n",
    "    # Note: This print is to make sure the client was created properly, to make sure if problems happen we can find out where what went wrong\n",
    "    if fred is None:\n",
    "        print(f\"FRED client is not initialized.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"[macro] Downloading FRED series {series_id} from {start} to {end} ...\")\n",
    "\n",
    "    try:\n",
    "        series = fred.get_series(series_id, observation_start=start, observation_end=end)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading series {series_id}:\", e)\n",
    "        return None\n",
    "\n",
    "    if series is None or series.empty:\n",
    "        print(f\"Data was not returned for series {series_id}\")\n",
    "        return None\n",
    "\n",
    "    # Note: Turning the series into a DataFrame with columns: date, value\n",
    "    df = series.to_frame(name=\"value\")\n",
    "    df.index.name = \"date\"\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_macro(df, name):\n",
    "    # Note: This function saves the macro DataFrame into data/raw/macro/\n",
    "    if df is None or df.empty:\n",
    "        print(f\"Nothing to save for {name}\")\n",
    "        return\n",
    "\n",
    "    out_path = os.path.join(RAW_MACRO_DIR, f\"{name}_fred_raw.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {name} data to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: Now we loop over all the FRED series we defined earlier in FRED_SERIES\n",
    "# Note: Keys are our friendly names (cpi, unemployment, etc.), values are the actual FRED IDs\n",
    "for name, series_id in FRED_SERIES.items():\n",
    "    df_macro = download_fred_series(series_id)\n",
    "    save_macro(df_macro, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df494ec",
   "metadata": {},
   "source": [
    "# Google Trends Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75458c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\ndef download_trends_keyword(keyword, start_date=TRENDS_START_DATE, end_date=TODAY):\\n    # Note: This function downloads the Google Trends \"interest over time\" for ONE keyword\\n    # Note: Google Trends needs the format \"YYYY-MM-DD YYYY-MM-DD\", all in one string\\n\\n    print(f\"Downloading Google Trends data for \\'{keyword}\\'...\")\\n\\n    try:\\n        # Note: Build the timeframe in the format Google Trends expects\\n        timeframe = f\"{start_date} {end_date}\"\\n\\n        # Note: IMPORTANT CHANGE:\\n        # Note: Instead of one global pytrends object, we create a fresh TrendReq() per keyword call.\\n        # Note: This sometimes helps when Google has rate-limited a previous session.\\n        local_pytrends = TrendReq(hl=\\'en-US\\', tz=360)\\n\\n        # Note: pytrends requires that we \"build a payload\" before fetching data\\n        local_pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo=\\'\\', gprop=\\'\\')\\n\\n        # Note: Now we can request the interest-over-time data.\\n        df = local_pytrends.interest_over_time()\\n\\n        if df.empty:\\n            print(f\"No data returned for keyword \\'{keyword}\\'\")\\n            return None\\n\\n        # Note: interest_over_time() returns a dataframe with the keyword and an \\'isPartial\\' column\\n        # Note: We only care about the keyword and the date, so we will clean it up a bit\\n        df = df.reset_index()\\n\\n        # Note: Some column names may have spaces or weird formatting, so we can clean that up too\\n        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\\n\\n        # Note: Will also drop the \\'ispartial\\' column since we don’t need it\\n        if \"ispartial\" in df.columns:\\n            df = df.drop(columns=[\"ispartial\"])\\n\\n        # Note: Also rename the keyword column to something predictable\\n        df = df.rename(columns={keyword.lower().replace(\" \", \"_\"): \"value\"})\\n\\n        return df\\n\\n    except Exception as e:\\n        print(f\"Error downloading keyword \\'{keyword}\\':\", e)\\n        return None\\n\\n\\ndef save_trends(df, keyword):\\n    # Note: This function saves the sentiment dataframe into data/raw/sentiment/\\n    if df is None or df.empty:\\n        print(f\"[sentiment] Nothing to save for \\'{keyword}\\'\")\\n        return\\n\\n    # Note: Just making sure to format the filename nicely, like: recession_trends_raw.csv\\n    filename = f\"{keyword.lower().replace(\\' \\', \\'_\\')}_trends_raw.csv\"\\n    out_path = os.path.join(RAW_SENTIMENT_DIR, filename)\\n\\n    df.to_csv(out_path, index=False)\\n    print(f\"Saved keyword \\'{keyword}\\' to {out_path} (rows: {len(df)})\")\\n\\n# Note: We are NOT looping through all TRENDS_KEYWORDS automatically anymore.\\n# Note: Instead, we will call download_trends_keyword() and save_trends() manually, one keyword at a time in separate cells.\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Now we move on to Google Trends sentiment data using pytrends\n",
    "# Note: Google Trends only goes back to 2004, which is fine\n",
    "# Note: We'll pull sentiment for these keywords: [\"recession\", \"stock market crash\", \"bear market\", \"financial crisis\"]\n",
    "\n",
    "# Note: Okay so first try got code 429 error, indicating that Google did not allow too many requests in a short span\n",
    "# Note: Basically what happened was it downloaded for some keywords and did not for some, indicating to many requests too quick basically\n",
    "# Note: Idea is to retry with a waittime basically\n",
    "# Note: Here's some notes, seems like it was not related to the time issue, since it still is not working with a workaround for 60s waittime\n",
    "# Note: Upon further research it seems like this has more to do with automated requests\n",
    "# Note: New idea is to now try one keyword at a time, with a fresh pytrends session for each call and manually control it\n",
    "\n",
    "# Note: Initialize the pytrends connection\n",
    "# Note: Can use any timezon but we'll use US for simplicity\n",
    "# pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def download_trends_keyword(keyword, start_date=TRENDS_START_DATE, end_date=TODAY):\n",
    "    # Note: This function downloads the Google Trends \"interest over time\" for ONE keyword\n",
    "    # Note: Google Trends needs the format \"YYYY-MM-DD YYYY-MM-DD\", all in one string\n",
    "\n",
    "    print(f\"Downloading Google Trends data for '{keyword}'...\")\n",
    "\n",
    "    try:\n",
    "        # Note: Build the timeframe in the format Google Trends expects\n",
    "        timeframe = f\"{start_date} {end_date}\"\n",
    "\n",
    "        # Note: IMPORTANT CHANGE:\n",
    "        # Note: Instead of one global pytrends object, we create a fresh TrendReq() per keyword call.\n",
    "        # Note: This sometimes helps when Google has rate-limited a previous session.\n",
    "        local_pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "        # Note: pytrends requires that we \"build a payload\" before fetching data\n",
    "        local_pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='', gprop='')\n",
    "\n",
    "        # Note: Now we can request the interest-over-time data.\n",
    "        df = local_pytrends.interest_over_time()\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data returned for keyword '{keyword}'\")\n",
    "            return None\n",
    "\n",
    "        # Note: interest_over_time() returns a dataframe with the keyword and an 'isPartial' column\n",
    "        # Note: We only care about the keyword and the date, so we will clean it up a bit\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Note: Some column names may have spaces or weird formatting, so we can clean that up too\n",
    "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "        # Note: Will also drop the 'ispartial' column since we don’t need it\n",
    "        if \"ispartial\" in df.columns:\n",
    "            df = df.drop(columns=[\"ispartial\"])\n",
    "\n",
    "        # Note: Also rename the keyword column to something predictable\n",
    "        df = df.rename(columns={keyword.lower().replace(\" \", \"_\"): \"value\"})\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading keyword '{keyword}':\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_trends(df, keyword):\n",
    "    # Note: This function saves the sentiment dataframe into data/raw/sentiment/\n",
    "    if df is None or df.empty:\n",
    "        print(f\"[sentiment] Nothing to save for '{keyword}'\")\n",
    "        return\n",
    "\n",
    "    # Note: Just making sure to format the filename nicely, like: recession_trends_raw.csv\n",
    "    filename = f\"{keyword.lower().replace(' ', '_')}_trends_raw.csv\"\n",
    "    out_path = os.path.join(RAW_SENTIMENT_DIR, filename)\n",
    "\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved keyword '{keyword}' to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "# Note: We are NOT looping through all TRENDS_KEYWORDS automatically anymore.\n",
    "# Note: Instead, we will call download_trends_keyword() and save_trends() manually, one keyword at a time in separate cells.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3522ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Now going to call it with one keyword at a time to see if that works\n",
    "# Note: Trying with \"recession\"\n",
    "\n",
    "# keyword = \"recession\"\n",
    "# df_trend = download_trends_keyword(keyword)\n",
    "# save_trends(df_trend, keyword)\n",
    "\n",
    "# Note: Funnily, tried wustl-encrypted, then my personal hotspot, and it did not but worked on eduroam\n",
    "# Note: Now this kernel will just go through the trendkeywords from the first kernel and download for each keyword\n",
    "# import time\n",
    "\n",
    "# for keyword in TRENDS_KEYWORDS:\n",
    "#     print(f\"Starting download for keyword: '{keyword}'\")\n",
    "\n",
    "#     df_trend = download_trends_keyword(keyword)\n",
    "#     save_trends(df_trend, keyword)\n",
    "\n",
    "#     # Note: Making sure to have a quick pause so Google does not stop us again, but this should not be an issue since we are starting a new session each time\n",
    "#     time.sleep(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28396e0f",
   "metadata": {},
   "source": [
    "## Change to the idea\n",
    "\n",
    "Google Trends is throwing 429 errors, and so for now we will scrap the idea of sentiment analysis, and move on with the rest of the code. We tried multiple devices to see if it was an IP thing, we also tried doing one keyword at a time restarting the kernel, or changing the wifi networks, also kept a timer in between but it seems to not be working. We will work with what we have and research more as we move on with the project to see what we can add on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee719ce",
   "metadata": {},
   "source": [
    "# Cleaning the Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f7d44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning price data for: sp500\n",
      "Saved cleaned file → /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/prices/sp500_prices_clean.csv (rows: 11561)\n",
      "\n",
      "Cleaning price data for: vix\n",
      "Saved cleaned file → /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/prices/vix_prices_clean.csv (rows: 9033)\n"
     ]
    }
   ],
   "source": [
    "# Note: Now we move on to CLEANING the price data, this includes both the S&P 500 (^GSPC) and the VIX (^VIX).\n",
    "# Note: The raw CSV files live in data/raw/prices/, and we will output cleaned versions into data/cleaned/prices/\n",
    "# Note: Cleaning steps will involve parsing the date column properly, lowercasing + underscoring column names, sorting chronologically, forward-filling missing values (common in VIX / holidays)\n",
    "# Note: And then we can save the clean CSVs\n",
    "\n",
    "def clean_price_file(raw_path, out_path):\n",
    "    df = pd.read_csv(raw_path)\n",
    "    # Note: Next step is to clean the columns, because the yfinance has weird tuple-like column names int he CSV files\n",
    "    # Note: Will clean the column names by stripping the parentheses, splitting on the commas, taking the first part, and also lowercase and underscoring\n",
    "    def clean_col_name(c):\n",
    "        c_str = str(c).strip()\n",
    "        if c_str.startswith(\"(\") and \",\" in c_str:\n",
    "            # Example: \"('close',_'^gspc')\" -> \"close\"\n",
    "            inner = c_str.strip(\"()\")\n",
    "            parts = inner.split(\",\")\n",
    "            base = parts[0].strip().strip(\"'\").strip('\"')\n",
    "            return base.lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            return c_str.lower().replace(\" \", \"_\")\n",
    "\n",
    "    df.columns = [clean_col_name(c) for c in df.columns]\n",
    "\n",
    "    # Note: Parse the date column (sometimes named \"date\")\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    else:\n",
    "        # Note: If the date is the index instead (chances of which are pretty low), we will take care of that by resetting the index\n",
    "        df = df.reset_index()\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Note: Sort data by date just to be safe\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # Note: Forward-fill missing values, this is very normal — holidays, partial trading days, etc.\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].ffill()\n",
    "\n",
    "    # Note: Save cleaned file\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved cleaned file → {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: These are our price files to clean\n",
    "PRICE_FILES = {\n",
    "    \"sp500\": \"sp500_prices_raw.csv\",\n",
    "    \"vix\": \"vix_prices_raw.csv\"\n",
    "}\n",
    "\n",
    "# Note: Loop through each raw file and run the function above to clean it\n",
    "for name, filename in PRICE_FILES.items():\n",
    "    raw_path = os.path.join(RAW_PRICES_DIR, filename)\n",
    "    out_path = os.path.join(CLEAN_PRICES_DIR, f\"{name}_prices_clean.csv\")\n",
    "\n",
    "    print(f\"\\nCleaning price data for: {name}\")\n",
    "    \n",
    "    if not os.path.exists(raw_path):\n",
    "        print(f\"Raw file does NOT exist → {raw_path}\")\n",
    "        continue\n",
    "\n",
    "    clean_price_file(raw_path, out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298181d1",
   "metadata": {},
   "source": [
    "# Cleaning the Macroeconomic data from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a66126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning macro data for: cpi\n",
      "Saved cleaned file for /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/macro/cpi_fred_clean.csv (rows: 549)\n",
      "\n",
      "Cleaning macro data for: unemployment\n",
      "Saved cleaned file for /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/macro/unemployment_fred_clean.csv (rows: 548)\n",
      "\n",
      "Cleaning macro data for: fed_funds_rate\n",
      "Saved cleaned file for /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/macro/fed_funds_rate_fred_clean.csv (rows: 550)\n",
      "\n",
      "Cleaning macro data for: m2_money_stock\n",
      "Saved cleaned file for /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned/macro/m2_money_stock_fred_clean.csv (rows: 549)\n"
     ]
    }
   ],
   "source": [
    "# Note: Now we move on to CLEANING the macroeconomic data we pulled from FRED.\n",
    "# Note: Raw macro files are in data/raw/macro/, and we will save cleaned versions into data/cleaned/macro/.\n",
    "# Note: The files (from our earlier download) should be: cpi_fred_raw.csv, unemployment_fred_raw.csv, fed_funds_rate_fred_raw.csv, m2_money_stock_fred_raw.csv\n",
    "# Note: Cleaning steps will involve: parsing the 'date' column, making sure 'value' is numeric, sorting by date, forward-filling missing macro values (which is very normal for monthly/irregular data)\n",
    "# Note: And then we can save it as *_fred_clean.csv\n",
    "# Note: The idea around the forward filling, as we did earlier too is in the case of missing values in a time series, its just to replace the last known valid value, using an inbuilt pandas function\n",
    "\n",
    "def clean_macro_file(raw_path, out_path):\n",
    "    df = pd.read_csv(raw_path)\n",
    "\n",
    "    # Note: Expecting columns: 'date' and 'value', from where we can parse 'date' to datetime\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    else:\n",
    "        print(f\"'date' column not found in {raw_path}.\")\n",
    "        return\n",
    "\n",
    "    # Note: Ensuring 'value' is numeric; also changing any non-numeric values to NaN\n",
    "    if \"value\" in df.columns:\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        print(f\"'value' column not found in {raw_path}.\")\n",
    "        return\n",
    "\n",
    "    # Note: Sorting by date\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # Note: Forward-filling any missing values (macro series often have gaps or delays)\n",
    "    df[\"value\"] = df[\"value\"].ffill()\n",
    "\n",
    "    # Note: Saving the cleaned file\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved cleaned file for {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: These will be our macro files to clean\n",
    "MACRO_FILES = {\n",
    "    \"cpi\": \"cpi_fred_raw.csv\",\n",
    "    \"unemployment\": \"unemployment_fred_raw.csv\",\n",
    "    \"fed_funds_rate\": \"fed_funds_rate_fred_raw.csv\",\n",
    "    \"m2_money_stock\": \"m2_money_stock_fred_raw.csv\",\n",
    "}\n",
    "\n",
    "# Note: Loop through each raw macro file and clean it\n",
    "for name, filename in MACRO_FILES.items():\n",
    "    raw_path = os.path.join(RAW_MACRO_DIR, filename)\n",
    "    out_path = os.path.join(CLEAN_MACRO_DIR, f\"{name}_fred_clean.csv\")\n",
    "\n",
    "    print(f\"\\nCleaning macro data for: {name}\")\n",
    "\n",
    "    if not os.path.exists(raw_path):\n",
    "        print(f\"Raw macro file does NOT exist for {raw_path}\")\n",
    "        continue\n",
    "\n",
    "    clean_macro_file(raw_path, out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b062b9e",
   "metadata": {},
   "source": [
    "# Merging everything into one, clean up columns too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efde3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging prices: (9033, 11)\n",
      "Merging macro variable: cpi\n",
      "Merging macro variable: unemployment\n",
      "Merging macro variable: fed_funds_rate\n",
      "Merging macro variable: m2_money_stock\n",
      "Final merged dataset saved to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/merged/merged_dataset_cleaned.csv\n",
      "Final shape: (9033, 15)\n",
      "        date  close_sp500  high_sp500   low_sp500  open_sp500  volume_sp500  \\\n",
      "0 1990-01-02   359.690002  359.690002  351.980011  353.399994     162070000   \n",
      "1 1990-01-03   358.760010  360.589996  357.890015  359.690002     192330000   \n",
      "2 1990-01-04   355.670013  358.760010  352.890015  358.760010     177000000   \n",
      "3 1990-01-05   352.200012  355.670013  351.350006  355.670013     158530000   \n",
      "4 1990-01-08   353.790009  354.239990  350.540009  352.200012     140110000   \n",
      "\n",
      "   close_vix   high_vix    low_vix   open_vix  volume_vix  cpi  unemployment  \\\n",
      "0  17.240000  17.240000  17.240000  17.240000           0  NaN           NaN   \n",
      "1  18.190001  18.190001  18.190001  18.190001           0  NaN           NaN   \n",
      "2  19.219999  19.219999  19.219999  19.219999           0  NaN           NaN   \n",
      "3  20.110001  20.110001  20.110001  20.110001           0  NaN           NaN   \n",
      "4  20.260000  20.260000  20.260000  20.260000           0  NaN           NaN   \n",
      "\n",
      "   fed_funds_rate  m2_money_stock  \n",
      "0             NaN             NaN  \n",
      "1             NaN             NaN  \n",
      "2             NaN             NaN  \n",
      "3             NaN             NaN  \n",
      "4             NaN             NaN  \n"
     ]
    }
   ],
   "source": [
    "# Note: This is the cell for cleaning up the column names and stuff from yfinance (tuple names) and outputs a perfectly structured dataset for analysis and also merging everything\n",
    "\n",
    "def load_clean_price(name):\n",
    "    # Note: Loading each cleaned price file by name (sp500 or vix)\n",
    "    path = os.path.join(CLEAN_PRICES_DIR, f\"{name}_prices_clean.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_clean_macro(name):\n",
    "    # Note: Loading each cleaned macro file by name\n",
    "    path = os.path.join(CLEAN_MACRO_DIR, f\"{name}_fred_clean.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Note: Loading price data ---\n",
    "df_sp500 = load_clean_price(\"sp500\")\n",
    "df_vix = load_clean_price(\"vix\")\n",
    "\n",
    "# Note: Renaming columns so they are clean and readable\n",
    "df_sp500 = df_sp500.rename(columns={\n",
    "    \"open\": \"open_sp500\",\n",
    "    \"high\": \"high_sp500\",\n",
    "    \"low\": \"low_sp500\",\n",
    "    \"close\": \"close_sp500\",\n",
    "    \"volume\": \"volume_sp500\",\n",
    "})\n",
    "\n",
    "df_vix = df_vix.rename(columns={\n",
    "    \"open\": \"open_vix\",\n",
    "    \"high\": \"high_vix\",\n",
    "    \"low\": \"low_vix\",\n",
    "    \"close\": \"close_vix\",\n",
    "    \"volume\": \"volume_vix\",\n",
    "})\n",
    "\n",
    "\n",
    "# Note: Merging the price data\n",
    "merged = pd.merge(df_sp500, df_vix, on=\"date\", how=\"inner\")\n",
    "print(\"After merging prices:\", merged.shape)\n",
    "\n",
    "\n",
    "# Note: Loading and merging the macro series data\n",
    "macro_series = [\"cpi\", \"unemployment\", \"fed_funds_rate\", \"m2_money_stock\"]\n",
    "\n",
    "for macro in macro_series:\n",
    "    df_macro = load_clean_macro(macro)\n",
    "    print(f\"Merging macro variable: {macro}\")\n",
    "\n",
    "    # Note: Renaming \"value\" to macro series name\n",
    "    df_macro = df_macro.rename(columns={\"value\": macro})\n",
    "\n",
    "    merged = pd.merge(\n",
    "        merged,\n",
    "        df_macro,\n",
    "        on=\"date\",\n",
    "        how=\"left\"     # Note: keep all daily dates\n",
    "    )\n",
    "\n",
    "\n",
    "# Note: Sorting + forward-filling macro values\n",
    "merged = merged.sort_values(\"date\").reset_index(drop=True)\n",
    "merged[macro_series] = merged[macro_series].ffill()\n",
    "\n",
    "\n",
    "# Note: Saving final merged dataset\n",
    "output_path = os.path.join(MERGED_DIR, \"merged_dataset_cleaned.csv\")\n",
    "merged.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final merged dataset saved to {output_path}\")\n",
    "print(f\"Final shape: {merged.shape}\")\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89250a2b",
   "metadata": {},
   "source": [
    "***One final note, for the macro dataset, interestingly that starts on February 1, instead of January 1, which is why it shows up as having no value for the first month of 1990, where as the prices start 1990 January 1.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
