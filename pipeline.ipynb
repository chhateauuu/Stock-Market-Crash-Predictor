{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11147fd",
   "metadata": {},
   "source": [
    "This file will cover the entire data pipeline process for acquiring all the data, cleaning, and processing for the sake of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d2a93",
   "metadata": {},
   "source": [
    "# Importing and configurations, also folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19809636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code\n",
      "Data directory: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data\n",
      "Raw data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw\n",
      "Cleaned data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/cleaned\n",
      "Merged data folder: /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/merged\n"
     ]
    }
   ],
   "source": [
    "import os # Note: for working with folders / paths\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf             # Note: To download stock/market data\n",
    "from fredapi import Fred          # Note: To download macroeconomic data from FRED\n",
    "from pytrends.request import TrendReq  # Note: To pull Google Trends sentiment data\n",
    "import duckdb                     # Note: This one for now is unsure-ish, can be used for storing/querying data with SQL\n",
    "\n",
    "# Note: This is the BASE_DIR = the folder where this notebook is running\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# # Note: This will be the main data directory:\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Note: Subfolders for different stages of the pipeline:\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "CLEANED_DIR = os.path.join(DATA_DIR, \"cleaned\")\n",
    "MERGED_DIR = os.path.join(DATA_DIR, \"merged\")\n",
    "\n",
    "# Note: Now also creating more specific subfolders for raw data, each housing different stuff:\n",
    "# Note: data/raw/prices/     -> raw price data (S&P 500, VIX, etc.)\n",
    "# Note: data/raw/macro/      -> raw macro data (CPI, unemployment, etc.)\n",
    "# Note: data/raw/sentiment/  -> raw sentiment data (Google Trends, etc.)\n",
    "RAW_PRICES_DIR = os.path.join(RAW_DIR, \"prices\")\n",
    "RAW_MACRO_DIR = os.path.join(RAW_DIR, \"macro\")\n",
    "RAW_SENTIMENT_DIR = os.path.join(RAW_DIR, \"sentiment\")\n",
    "\n",
    "# Note: Same structure for cleaned data:\n",
    "# Note: data/cleaned/prices/\n",
    "# Note: data/cleaned/macro/\n",
    "# Note: data/cleaned/sentiment/\n",
    "CLEAN_PRICES_DIR = os.path.join(CLEANED_DIR, \"prices\")\n",
    "CLEAN_MACRO_DIR = os.path.join(CLEANED_DIR, \"macro\")\n",
    "CLEAN_SENTIMENT_DIR = os.path.join(CLEANED_DIR, \"sentiment\")\n",
    "\n",
    "# Note: We want historical data starting from 1980, so setting that as a golbal variable\n",
    "START_DATE = \"1980-01-01\"\n",
    "\n",
    "# Note: Today's date (so we always pull up-to-date data)\n",
    "TODAY = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Note: These will be the financial tickers (for yfinance)\n",
    "# Note: ^GSPC = S&P 500 index; ^VIX  = Volatility Index\n",
    "TICKERS = {\n",
    "    \"sp500\": \"^GSPC\",\n",
    "    \"vix\": \"^VIX\",\n",
    "}\n",
    "\n",
    "# Note: Next we have the FRED macroeconomic series IDs\n",
    "# Note: These IDs come from the FRED website\n",
    "FRED_SERIES = {\n",
    "    \"cpi\": \"CPIAUCSL\",           # Note: Consumer Price Index\n",
    "    \"unemployment\": \"UNRATE\",    # Note: Unemployment rate\n",
    "    \"fed_funds_rate\": \"FEDFUNDS\",\n",
    "    \"m2_money_stock\": \"M2SL\",\n",
    "}\n",
    "\n",
    "# Note: Finally the Google Trends sentiment settings\n",
    "# Note: These are some of the keywords we care about for sentiment around crashes/recessions.\n",
    "TRENDS_KEYWORDS = [\n",
    "    # \"recession\",\n",
    "    # \"stock market crash\",\n",
    "    \"bear market\",\n",
    "    \"financial crisis\",\n",
    "    \"economic crisis\",\n",
    "    \"market crash\",\n",
    "    \"economic collapse\",\n",
    "    \"market panic\",\n",
    "    \"market volatility\",\n",
    "    \"stock crash\",\n",
    "    \"credit crunch\",\n",
    "    \"bank run\",\n",
    "    \"bank collapse\",\n",
    "    \"investor panic\",\n",
    "    \"yield curve inversion\",\n",
    "    \"inflation crisis\",\n",
    "    \"stagflation\",\n",
    "    \"should I sell my stocks\",\n",
    "    \"why is the stock market dropping\",\n",
    "    \"market uncertainty\",\n",
    "    \"fed meeting\",\n",
    "    \"inflation report\",\n",
    "    \"jobs report\"\n",
    "]\n",
    "# Note: Google Trends only goes back to 2004, so our sentiment data starts then.\n",
    "TRENDS_START_DATE = \"2004-01-01\"\n",
    "\n",
    "# Note: Let's print it so we can make sure most of this configuration and folder setup worked\n",
    "print(\"Base directory:\", BASE_DIR)\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "print(\"Raw data folder:\", RAW_DIR)\n",
    "print(\"Cleaned data folder:\", CLEANED_DIR)\n",
    "print(\"Merged data folder:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f5340",
   "metadata": {},
   "source": [
    "# Acquire price data (S&P 500 + VIX) by downloading all of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a64fcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/q8ls960x2513d35yd59cvhc00000gn/T/ipykernel_54145/2921958070.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/var/folders/bv/q8ls960x2513d35yd59cvhc00000gn/T/ipykernel_54145/2921958070.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prices] Downloading sp500 (^GSPC) from 1980-01-01 to 2025-11-12 ...\n",
      "[prices] Saved sp500 data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/prices/sp500_prices_raw.csv (rows: 11561)\n",
      "[prices] Downloading vix (^VIX) from 1980-01-01 to 2025-11-12 ...\n",
      "[prices] Saved vix data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/prices/vix_prices_raw.csv (rows: 9033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: This is where we will download the actual price data using yfinance, this includes the S&P 500 (^GSPC) and the VIX index (^VIX)\n",
    "# Note: We'll pull all data from our START_DATE (1980) up through TODAY\n",
    "# Note: The print statements below are practices we are adopting so as to make sure at each step we understand what is causing an error if something happens for debugging\n",
    "\n",
    "def download_prices(name, ticker, start=START_DATE, end=TODAY):\n",
    "    # Note: This function downloads price data for a given ticker.\n",
    "    # Note: 'name' is just our friendly label like \"sp500\".\n",
    "    # Note: 'ticker' is the actual Yahoo Finance symbol like \"^GSPC\".\n",
    "\n",
    "    print(f\"[prices] Downloading {name} ({ticker}) from {start} to {end} ...\")\n",
    "\n",
    "    # Note: yf.download() pulls OHLCV data (Open, High, Low, Close, Volume)\n",
    "    df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "    # Note: If somehow yfinance returns nothing, we don't want the code to break.\n",
    "    if df.empty:\n",
    "        print(f\"[prices] WARNING: no data returned for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    # Note: So first gotta convert all column names into plain strings because one of the column names in yfinance seem to not be str but a tuple\n",
    "    df.columns = df.columns.map(str)\n",
    "\n",
    "    # Note: Moving the index into a normal 'date' column and cleaning column names a bit.\n",
    "    df = df.reset_index()\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_prices(df, name):\n",
    "    # Note: This function simply saves the dataframe to the correct folder.\n",
    "    # Note: If df is None or empty, then there's nothing to save.\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(f\"[prices] Nothing to save for {name}\")\n",
    "        return\n",
    "\n",
    "    # Note: Construct the path like: data/raw/prices/sp500_prices_raw.csv\n",
    "    out_path = os.path.join(RAW_PRICES_DIR, f\"{name}_prices_raw.csv\")\n",
    "\n",
    "    # Note: Saving all of the data\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"[prices] Saved {name} data to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: Now we loop through all tickers we defined earlier (S&P 500 + VIX)\n",
    "# Note: For each ticker, we download it and then save it.\n",
    "for name, ticker in TICKERS.items():\n",
    "    df = download_prices(name, ticker)\n",
    "    save_prices(df, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a61d5",
   "metadata": {},
   "source": [
    "# Get the macro data from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2dfc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to FRED\n",
      "[macro] Downloading FRED series CPIAUCSL from 1980-01-01 to 2025-11-12 ...\n",
      "Saved cpi data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/cpi_fred_raw.csv (rows: 549)\n",
      "[macro] Downloading FRED series UNRATE from 1980-01-01 to 2025-11-12 ...\n",
      "Saved unemployment data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/unemployment_fred_raw.csv (rows: 548)\n",
      "[macro] Downloading FRED series FEDFUNDS from 1980-01-01 to 2025-11-12 ...\n",
      "Saved fed_funds_rate data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/fed_funds_rate_fred_raw.csv (rows: 550)\n",
      "[macro] Downloading FRED series M2SL from 1980-01-01 to 2025-11-12 ...\n",
      "Saved m2_money_stock data to /Users/aaryabratc/Desktop/WASHU MAIN/FALL 2025/cse3104_Data_Manipulation_and_Management/Project/project_code/data/raw/macro/m2_money_stock_fred_raw.csv (rows: 549)\n"
     ]
    }
   ],
   "source": [
    "# Note: For loading the .env variables like API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Note: Already went to the FRED website and got an API key, which is in the .env (so git won't track it), so will use that for this step\n",
    "# Note: Now we move on to downloading macroeconomic data from FRED\n",
    "# Note: This will give us things like CPI, unemployment, Fed funds rate, and M2 money stock\n",
    "\n",
    "# Note: Just pulling that API key from the .env\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Note: Now we will create the FRED client using the key\n",
    "fred = None\n",
    "try:\n",
    "    fred = Fred(api_key=FRED_API_KEY)\n",
    "    print(\"Successfully connected to FRED\")\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to FRED:\", e)\n",
    "    fred = None\n",
    "\n",
    "\n",
    "def download_fred_series(series_id, start=START_DATE, end=TODAY):\n",
    "    # Note: This function downloads ONE macro time series from FRED, example series_id values would be like \"CPIAUCSL\", \"UNRATE\", etc\n",
    "\n",
    "    # Note: This print is to make sure the client was created properly, to make sure if problems happen we can find out where what went wrong\n",
    "    if fred is None:\n",
    "        print(f\"FRED client is not initialized.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"[macro] Downloading FRED series {series_id} from {start} to {end} ...\")\n",
    "\n",
    "    try:\n",
    "        series = fred.get_series(series_id, observation_start=start, observation_end=end)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading series {series_id}:\", e)\n",
    "        return None\n",
    "\n",
    "    if series is None or series.empty:\n",
    "        print(f\"Data was not returned for series {series_id}\")\n",
    "        return None\n",
    "\n",
    "    # Note: Turning the series into a DataFrame with columns: date, value\n",
    "    df = series.to_frame(name=\"value\")\n",
    "    df.index.name = \"date\"\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_macro(df, name):\n",
    "    # Note: This function saves the macro DataFrame into data/raw/macro/\n",
    "    if df is None or df.empty:\n",
    "        print(f\"Nothing to save for {name}\")\n",
    "        return\n",
    "\n",
    "    out_path = os.path.join(RAW_MACRO_DIR, f\"{name}_fred_raw.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {name} data to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: Now we loop over all the FRED series we defined earlier in FRED_SERIES\n",
    "# Note: Keys are our friendly names (cpi, unemployment, etc.), values are the actual FRED IDs\n",
    "for name, series_id in FRED_SERIES.items():\n",
    "    df_macro = download_fred_series(series_id)\n",
    "    save_macro(df_macro, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df494ec",
   "metadata": {},
   "source": [
    "# Google Trends Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75458c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Now we move on to Google Trends sentiment data using pytrends\n",
    "# Note: Google Trends only goes back to 2004, which is fine\n",
    "# Note: We'll pull sentiment for these keywords: [\"recession\", \"stock market crash\", \"bear market\", \"financial crisis\"]\n",
    "\n",
    "# Note: Okay so first try got code 429 error, indicating that Google did not allow too many requests in a short span\n",
    "# Note: Basically what happened was it downloaded for some keywords and did not for some, indicating to many requests too quick basically\n",
    "# Note: Idea is to retry with a waittime basically\n",
    "# Note: Here's some notes, seems like it was not related to the time issue, since it still is not working with a workaround for 60s waittime\n",
    "# Note: Upon further research it seems like this has more to do with automated requests\n",
    "# Note: New idea is to now try one keyword at a time, with a fresh pytrends session for each call and manually control it\n",
    "\n",
    "# Note: Initialize the pytrends connection\n",
    "# Note: Can use any timezon but we'll use US for simplicity\n",
    "# pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "\n",
    "def download_trends_keyword(keyword, start_date=TRENDS_START_DATE, end_date=TODAY):\n",
    "    # Note: This function downloads the Google Trends \"interest over time\" for ONE keyword\n",
    "    # Note: Google Trends needs the format \"YYYY-MM-DD YYYY-MM-DD\", all in one string\n",
    "\n",
    "    print(f\"Downloading Google Trends data for '{keyword}'...\")\n",
    "\n",
    "    try:\n",
    "        # Note: Build the timeframe in the format Google Trends expects\n",
    "        timeframe = f\"{start_date} {end_date}\"\n",
    "\n",
    "        # Note: IMPORTANT CHANGE:\n",
    "        # Note: Instead of one global pytrends object, we create a fresh TrendReq() per keyword call.\n",
    "        # Note: This sometimes helps when Google has rate-limited a previous session.\n",
    "        local_pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "        # Note: pytrends requires that we \"build a payload\" before fetching data\n",
    "        local_pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='', gprop='')\n",
    "\n",
    "        # Note: Now we can request the interest-over-time data.\n",
    "        df = local_pytrends.interest_over_time()\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data returned for keyword '{keyword}'\")\n",
    "            return None\n",
    "\n",
    "        # Note: interest_over_time() returns a dataframe with the keyword and an 'isPartial' column\n",
    "        # Note: We only care about the keyword and the date, so we will clean it up a bit\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Note: Some column names may have spaces or weird formatting, so we can clean that up too\n",
    "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "        # Note: Will also drop the 'ispartial' column since we donâ€™t need it\n",
    "        if \"ispartial\" in df.columns:\n",
    "            df = df.drop(columns=[\"ispartial\"])\n",
    "\n",
    "        # Note: Also rename the keyword column to something predictable\n",
    "        df = df.rename(columns={keyword.lower().replace(\" \", \"_\"): \"value\"})\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading keyword '{keyword}':\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_trends(df, keyword):\n",
    "    # Note: This function saves the sentiment dataframe into data/raw/sentiment/\n",
    "    if df is None or df.empty:\n",
    "        print(f\"[sentiment] Nothing to save for '{keyword}'\")\n",
    "        return\n",
    "\n",
    "    # Note: Just making sure to format the filename nicely, like: recession_trends_raw.csv\n",
    "    filename = f\"{keyword.lower().replace(' ', '_')}_trends_raw.csv\"\n",
    "    out_path = os.path.join(RAW_SENTIMENT_DIR, filename)\n",
    "\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved keyword '{keyword}' to {out_path} (rows: {len(df)})\")\n",
    "\n",
    "\n",
    "# Note: IMPORTANT CHANGE:\n",
    "# Note: We are NOT looping through all TRENDS_KEYWORDS automatically anymore.\n",
    "# Note: Instead, we will call download_trends_keyword() and save_trends() manually, one keyword at a time in separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3522ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download for keyword: 'bear market'\n",
      "Downloading Google Trends data for 'bear market'...\n",
      "Error downloading keyword 'bear market': The request failed: Google returned a response with code 429\n",
      "[sentiment] Nothing to save for 'bear market'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m save_trends(df_trend, keyword)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Note: Making sure to have a quick pause so Google does not stop us again, but this should not be an issue since we are starting a new session each time\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Note: Now going to call it with one keyword at a time to see if that works\n",
    "# Note: Trying with \"recession\"\n",
    "\n",
    "# keyword = \"recession\"\n",
    "# df_trend = download_trends_keyword(keyword)\n",
    "# save_trends(df_trend, keyword)\n",
    "\n",
    "# Note: Funnily, tried wustl-encrypted, then my personal hotspot, and it did not but worked on eduroam\n",
    "# Note: Now this kernel will just go through the trendkeywords from the first kernel and download for each keyword\n",
    "import time\n",
    "\n",
    "for keyword in TRENDS_KEYWORDS:\n",
    "    print(f\"Starting download for keyword: '{keyword}'\")\n",
    "\n",
    "    df_trend = download_trends_keyword(keyword)\n",
    "    save_trends(df_trend, keyword)\n",
    "\n",
    "    # Note: Making sure to have a quick pause so Google does not stop us again, but this should not be an issue since we are starting a new session each time\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d44a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a66126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
